{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Richardson method\n",
    "\n",
    "$$P(x^{(k+1)}-x^{(k)} = \\alpha_kr^{(k)}, k \\geq 0$$\n",
    "\n",
    "There are two version of the Richardson method:\n",
    "\n",
    "\n",
    "* the **Stationary case**, where $\\alpha_k$ is equal to a constant.\n",
    "\n",
    "\n",
    "* the **Dynamic case**, where $\\alpha_k$ is updated for each iteration.\n",
    "\n",
    "The matrix $P$ is the preconditioner. It should be a triangular or a diagonal matrix, in order to make the resulting matrix easy to compute.\n",
    "\n",
    "From the original formula of the Richardson method, we obtain that\n",
    "\n",
    "$$X^{k+1} = X^k (I-\\alpha_KP^{-1}A)+\\alpha_KP^{-1}b$$\n",
    "\n",
    "In this formula, $I-\\alpha_KP^{-1}A$ represents $B = R_\\alpha$\n",
    "\n",
    "We can find the set of eigenvalues of this matrix as follows\n",
    "\n",
    "$$\\tilde{\\lambda}(R_\\alpha) = 1 - \\alpha\\lambda_i$$\n",
    "\n",
    "since 1 is the eigenvalue of I and $\\alpha\\lambda_i$ are the eigenvalues of $\\alpha_KP^{-1}A$. We compute the spectral radius as\n",
    "\n",
    "$$\\rho(\\hat{x}) = max|1 - \\alpha\\lambda_i|$$\n",
    "\n",
    "As shown in slide 19, this leads to the proof that the optimal value for $\\alpha = \\frac{2}{\\lambda_{min}+\\lambda_{max}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradient Method\n",
    "\n",
    "The conjugate gradient method is a very efficient method used to solve linear systems for $A$ and $P$ which are **symmetric positive definite**.\n",
    "\n",
    "Gradient methods are also known as descent method because it approximates the solutions of the linear system by moving step by step towards the solution.\n",
    "\n",
    "Even though it was conceived as a gradient method, it is truly a one-shot method since to obtain the solution one should simply iterate a fixed number of times (that is, the size of the matrix A).\n",
    "\n",
    "The difference between the gradient and the conjugate gradient is given by the fact that in the latter you can reach the minimum faster thanks to an additional degree of freedom, which allows a movement towards the minimum which is not constrained by mandatory orthogonality.\n",
    "\n",
    "**This method converges more rapidly than Gauss-Seidel and Jacobi ones for s.p.d. linear systems.**\n",
    "\n",
    "\n",
    "## Some observations on iterative methods\n",
    "\n",
    "* If error and residual values are very different from each other, the matrix is probably heavily ill-conditioned and something is going wrong.\n",
    "\n",
    "* The number of operations and memory used by conjugate gradient are way smaller than those used by a Cholesky factorization method if the size of the matrix is large.\n",
    "\n",
    "* We can use factorization in order to obtain a pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
