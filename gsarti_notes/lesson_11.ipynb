{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow-up on Linear Systems\n",
    "\n",
    "Checking the eigenvalues of the resulting matrix is a practice called **spectral analysis**.\n",
    "\n",
    "We should adopt a **conservative approach** when computing LU, by storing the factorization $A = LU$ to avoid computing it again and thus reducing the computation by $\\frac{2}{3}n^3$. This is a necessary complement to **algorithmic optimization**.\n",
    "\n",
    "To use the **pivoting technique** (exchanging rows/columns of the system to avoid 0 pivots) we can apply a **permutation matrix**. The idea is to replace $LU$ with a $PA$ system, with $P$ being the **global permutation matrix** that takes in account all the permutations by row. This is equivalent to say that \n",
    "\n",
    "$$PAx = Pb \\implies Ly = Pb , \\;\\;\\; Ux = y$$\n",
    "\n",
    "**IMPORTANT: Do not forget to multiply by P also the right hand side, else you'll be solving a completely different problem!**\n",
    "\n",
    "If we use complete pivoting, we can rewrite the previous formula adding Q, which is the **permutation matrix for columns**.\n",
    "\n",
    "$$PAQ = LU$$\n",
    "\n",
    "$$PAQQ^{-1}x = Pb \\implies PAQ = LU, \\;\\;\\; Q^{-1}x = x^{*}$$\n",
    "\n",
    "$$Ly = Pb \\implies Ux^{*} = y \\implies x = Qx^{*}$$\n",
    "\n",
    "The operation of reconverting $x^{*}$ to x is the application of **reordering** in order to obtain the original results.\n",
    "\n",
    "The Binet theorem says that:\n",
    "\n",
    "$$det(A) = det(L)det(U) = det(U) = \\Pi_{k = 1}^{n} u_{kk}$$\n",
    "\n",
    "Since $det(L) = 1$ because $L$ is triangular matrix with 1's on the diagonal. Thus the computation of the determinant of a square matrix requires $O(n^3)$ operations.\n",
    "\n",
    "Regarding **matrix inversion**, we can solve the n systems for which $x^{1...n}$ is equal to the inverse matrix i-th column, and the right hand side is equal to the i-th column of the identity matrix $I$. This allows us to obtain the inverse of A in $2n^3$ operations, which is quite fast.\n",
    "\n",
    "The **Cholesky factorization** obtains a matrix $R$ from the original matrix $A$ in the system $Ax = b$ such that:\n",
    "\n",
    "$$A = R^TR$$\n",
    "\n",
    "Cholesky's factorization needs only half the operations of an LU factorization ($\\frac{n^3}{3} instead of \\frac{2n^3}{3}$) because the R matrix is the same, just transposed, instead of two L and U matrices.\n",
    "\n",
    "When computing the R matrix through Cholesky factorization, we can observe a **fill-in phenomenon** in which a sparse matrix A becomes a quite-dense matrix R. We can reduce this phenomenon through **re-ordering**, to avoid bigger memory usage. This allows to accumulate elements around the diagonal , creating a so-called **band** or **fuse matrix** which fits much easier in memory.\n",
    "\n",
    "The **Hilbert matrix** given by the equation\n",
    "\n",
    "$$H_{ij} = \\frac{1}{i + j - 1},\\;\\;\\; i,j = 1,\\dots,n$$\n",
    "\n",
    "is very ill-conditioned since the denominator grows to a very large number for a large version of the matrix, making the error grow incontrollably (reaches 100% of relative error at n = 14, when the condition number is $K_2(A) = 10^{18}$)\n",
    "\n",
    "In symmetric definite positive matrices, the **condition number** or **conditioning** is the ratio between the **maximum and minimum eigenvalue**. \n",
    "\n",
    "$$K(M) = \\frac{\\lambda_{max}(M)}{\\lambda_{min}(M)}$$\n",
    "\n",
    "The application of a transformation to a matrix in order to make it more solvable by numerical methods is called **preconditioning**. The transformation is called **preconditioner**.\n",
    "\n",
    "**Pay particular attention in understanding slides 74-75-76 of Slides-7a**\n",
    "\n",
    "The difference $\\|x - \\hat{x}\\| = \\|A^{-1}r\\|$ is clearly an error because if we multiply both members by A we have that Ax (= b) - the approximated solution of the system $A\\hat{x}$, the result is $AA^{-1}r$, which is simply the residuals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
